---
title: 2-9-2021
whiteboard_img: whiteboard-sa-2-9-2021.png
---

Topics covered, even if only ever so briefly:

## CYBR network security analytics class project

Got you set up with topics and projects. We used [this google doc](https://docs.google.com/document/d/1KRIX5B9SdTs0IakeCadjKLLq8gLljILTryYd_GG4sfM/edit),
which has a built-in template for describing your project.

## Experiential projects class

Your experiential projects class has you working with datasets with columns with huge cardinality, and the fields are not coded in a way that makes binning reasonable.
A cool idea is to create new features based on the `.value_counts()` of the factor with large cardinality. Looking at the distribution, you could create new binary (or ordinal!) features
binning the feature values -- something like `appears_more_than_once`, or `appearance_rate`.
Or! You could create a new column that is simply the number of times that the value for the given row-column appears in the dataset, and it could be used in a regression as opposed to a
dummy coded whatever. That value could be scaled to be between 0 and 1, so that a value of 1 would be "relatively, ordered the most" while a value close to 0 would be "relatively, hardly
ever ordered." The point is, there's probably not much useful information in 44k unique values... so make something new!

## Naive Bayes

Important to realize that it makes a probability prediction for _each class_. While its individual predictions are garbage because of the naivety of the assumptions,
the class probabilities can be compared, and the "most likely" class chosen.

Also note that Naive Bayes is an incremental learner...

## Incremental Learning

When large datasets and memory are a problem, incremental learners can help. [All Tensorflow and Keras models support incremental learning by default.](https://scikit-learn.org/stable/computing/scaling_strategies.html#scaling-strategies), if you want to go that route. Also, there are a few sklearn modeling
algorithms that do, too, [see the sklearn documentation on out-of-core learning](https://scikit-learn.org/stable/computing/scaling_strategies.html#scaling-strategies).

## Text Mining

Models can't handle text -- they can only handle vectors (points) floating in impossible-to-visualize spaces. So text mining
converts text to vectors! The book gives _document retrieval_ -- search engines -- as an example of text query-document distance
measuring, and also, as soon as text is in vector-form, the text can then be fed into any ML business problem, such as SPAM/HAM
classification.

## Choosing an optimal cutoff threshold

Decision boundaries of 0.5 are lazy -- instead, when deploying a model, consider choosing a cutoff threshold that
optimizes some metric. For example, the threshold that corresponds to the ROC curve point that is closest to the upper-left
can be found using algorithm such as the following:

~~~python
def find_optimal_cutoff_closest_to_perfect(fpr, tpr):
  """ Tries to find the best threshold to select on ROC.
  """
  distance = float("inf")
  threshold_index = -1
  for i in range(len(fpr)):
    d1 = math.sqrt((fpr[i] - 0)**2 + (tpr[i] - 1)**2)
    if d1 < distance:
      distance = d1
      threshold_index = i

  return threshold_index

fpr, tpr, thresholds = roc_curve(test_y, test_scores)
threshold_index = find_optimal_cutoff_closest_to_perfect(fpr, tpr)
optimal_threshold = thresholds[threshold_index]
~~~


I think that this is conceptually the same as the harmonic mean of the TPR and the TNR.
Another approach would be to optimize the F1 score (harmonic mean of PPV and TPR).
Whatever you do, _don't_ optimize either TPR or FPR -- these will always be optimal
when the other is at its most abysmal!


## Conceptual discussion of deployment, serialization

* Everything in python is an object, even functions
* Objects can be _serialized_

So, serialize a function that uses a fitted model to make a classification. Use
a function like [Cloudpickle](https://github.com/cloudpipe/cloudpickle).

## Handling outliers, standardization in sklearn

A student mentioned that they had PCA variables that looked like they hadn't been standardized,
and that they had some major outliers.
We practied know-only-a-little-bit googling, trying phrases like "sklearn standardize",
found our way to [the sklearn preprocessing documentation page](https://scikit-learn.org/stable/modules/preprocessing.html).
We read at the top about "robust scalers" which don't need to have outliers removed
before scaling.

sklearn [robust scaler](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#robustscaler) does not clip or remove the outliers, but [quantiletransformer](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#quantiletransformer-uniform-output)
does clip outliers.

## Creating new filler data

Pandas:

~~~python
df['new_column'] = "red" # fill all values with zero
df['new_column'].iloc[::3] = "blue" # every third row will be "blue"
df['new_column'].iloc[1::3] = "green" # every third row, starting with the second (because 0-based indexing), will be "green"
~~~

R:

idk rn
